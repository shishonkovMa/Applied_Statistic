{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import itertools\n",
    "\n",
    "from statsmodels.stats.descriptivestats import sign_test\n",
    "from statsmodels.stats.weightstats import zconfint\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPU_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>76057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>81542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>82702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>84566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>88682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CPU_seconds\n",
       "0              3\n",
       "1             33\n",
       "2            146\n",
       "3            227\n",
       "4            342\n",
       "..           ...\n",
       "131        76057\n",
       "132        81542\n",
       "133        82702\n",
       "134        84566\n",
       "135        88682\n",
       "\n",
       "[136 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failure_times = pd.read_csv('failure times.txt', header = None)\n",
    "failure_times.columns = ['CPU_seconds']\n",
    "failure_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []\n",
    "for i in range(1, len(failure_times.CPU_seconds)):\n",
    "    sample.append(failure_times.CPU_seconds[i] - failure_times.CPU_seconds[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset gives the failure times (in CPU seconds, measured in terms of execution time) of a real-time command and control software system.\n",
    "\n",
    "Do failures on average happen more often than every 500 CPU seconds? Let's test the following hypothesis:\n",
    "\n",
    "H0: average time between failures is not greater than 500 CPU seconds\n",
    "\n",
    "H1: average time between failures is greater than 500 CPU seconds\n",
    "\n",
    "First, let's use Student's t-test. What is its p-value? Round the answer to 4 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T test: Ttest_1sampResult(statistic=1.7572536270462775, pvalue=0.040579209207804064)\n"
     ]
    }
   ],
   "source": [
    "print('T test:', sc.stats.ttest_1samp(sample, 500, alternative='greater'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the same hypothesis and alternative with sign test. First of all, what number of observations in the sample is above 500?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "above 49\n",
      "not above 86\n"
     ]
    }
   ],
   "source": [
    "sample_gt_500 = list(filter(lambda x: x > 500, sample))\n",
    "sample_not_gt_500 = list(filter(lambda x: x <= 500, sample))\n",
    "print('above', len(sample_gt_500))\n",
    "print('not above', len(sample_not_gt_500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the p-value of the sign test? Round the answer to 4 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995002578123924"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stats.binom_test(49, 135, alternative='greater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, let's try signed rank test now. What p-value does it give? Provide the answer rounded to 4 decimal points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed rank test: 0.8632079654217537\n"
     ]
    }
   ],
   "source": [
    "new_sample = []\n",
    "for i in sample:\n",
    "    new_sample.append(i - 500)\n",
    "print(\"Signed rank test:\", 1 - sc.stats.wilcoxon(new_sample, zero_method='zsplit', correction=False)[1]/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's proceed to the permutation test with sum of the (centered) sample as a statistic. What is it's p-value? Round the answer to 4 decimal points.\n",
    "\n",
    "The sample is too big to go through all the permutation – let's use 10000 of them. To get the same result as us, use the functions from the example notebook, and set random seed = 0 before calling permutation_test_1s function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 21179, 'p': 0.0366}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def permutation_t_stat_1s(sample, mean):\n",
    "    t_stat = sum(sample - mean)\n",
    "    return t_stat\n",
    "\n",
    "def permutation_null_distr_1s(sample, mean, max_permutations = None):\n",
    "    centered_sample = sample - mean\n",
    "    if max_permutations:\n",
    "        signs_array = set([tuple(x) for x in 2 * np.random.randint(2, size = (max_permutations, \n",
    "                                                                              len(sample))) - 1 ])\n",
    "    else:\n",
    "        signs_array =  itertools.product([-1, 1], repeat = len(sample))\n",
    "    distr = [permutation_t_stat_1s(centered_sample * np.array(signs), 0) for signs in signs_array]\n",
    "    return distr\n",
    "\n",
    "def permutation_test_1s(sample, mean, max_permutations = None, alternative = 'two-sided', return_distr = False):\n",
    "    if alternative not in ('two-sided', 'less', 'greater'):\n",
    "        raise ValueError(\"alternative not recognized\\n\"\n",
    "                         \"should be 'two-sided', 'less' or 'greater'\")\n",
    "    \n",
    "    t_stat = permutation_t_stat_1s(sample, mean)\n",
    "    \n",
    "    null_distr = permutation_null_distr_1s(sample, mean, max_permutations)\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        p = sum([1. if abs(x) >= abs(t_stat) else 0. for x in null_distr]) / len(null_distr)\n",
    "    elif alternative == 'less':\n",
    "        p = sum([1. if x <= t_stat else 0. for x in null_distr]) / len(null_distr)\n",
    "    else: # alternative == 'greater':\n",
    "        p = sum([1. if x >= t_stat else 0. for x in null_distr]) / len(null_distr)\n",
    "        \n",
    "    if return_distr:\n",
    "        return {'t': t_stat, 'p': p, 'null_distr': null_distr}\n",
    "    else:\n",
    "        return {'t': t_stat, 'p': p}\n",
    "    \n",
    "np.random.seed(0)\n",
    "res = permutation_test_1s(pd.DataFrame(sample, columns=['cpu']).cpu, 500, max_permutations = 10000, alternative='greater')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the underlying assumptions of each test – which of the four do you think could be trusted for this problem?\n",
    "\n",
    "**Answer:**\n",
    "- Sign test\n",
    "- Student's t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
